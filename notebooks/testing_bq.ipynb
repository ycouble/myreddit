{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_path = \"/Users/yco/.dbt/dbt-user-creds.json\"\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path#, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from textacy import preprocessing\n",
    "\n",
    "import html\n",
    "import re\n",
    "\n",
    "def replace_md_url(text):\n",
    "    return re.compile(\n",
    "        r\"\\[(.*)\\]\\(([^\\(\\)\\[\\]]*)\\)\"\n",
    "    ).sub(r'\\g<1> \\g<2>', text)\n",
    "\n",
    "preproc_pipe = preprocessing.make_pipeline(\n",
    "    html.unescape,\n",
    "    html.unescape,\n",
    "    preprocessing.normalize.whitespace,\n",
    "    preprocessing.normalize.bullet_points,\n",
    "    preprocessing.normalize.hyphenated_words,\n",
    "    replace_md_url,\n",
    "    preprocessing.replace.urls,\n",
    "    preprocessing.remove.brackets,\n",
    "    preprocessing.normalize.unicode,\n",
    "    preprocessing.remove.accents,\n",
    "    preprocessing.replace.emojis,\n",
    "    preprocessing.replace.numbers,\n",
    "    preprocessing.replace.user_handles,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'ruvj9n',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'API',\n",
       "  'text': \"I'm making an ESG stock analysis program in Java, and so far the only free ESG API I've come across is ESGEnterprise, but I'm having trouble retrieving the data. Has anyone had any success/have any recs for other ESG APIs out there.\"},\n",
       " {'id': 'rup1uj',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'API',\n",
       "  'text': 'Hey everyone! I’m one of the creators of Sieve _URL_ and I’m excited to be sharing it!\\nSieve _URL_ **is an API that helps you turn petabyte-scale video data into a high-quality dataset, automatically.**\\nIt helps store, process, and semantically search your video data. Just think _NUMBER_ cameras recording footage at _NUMBER_ FPS, _NUMBER_/_NUMBER_. That would be _NUMBER_ million frames generated in a single day. The videos might be searchable by timestamp, but finding moments of interest is like searching for a needle in a haystack. Sieve tags useful attributes like people, motion, lighting, etc on every frame!\\nWe built this visual demo (link here _URL_ a little while back which we’d love to get feedback on. It’s \\\\~_NUMBER_ hours of security footage that our API processed in <_NUMBER_ mins and has simple querying and export functionality enabled. We see applications in better understanding what data you have, figuring out which data to send to labeling, sampling datasets for training, and building multiple test sets for models by scenario.\\nTo try it on your videos: _URL_ _URL_\\nVisual dashboard walkthrough: Click on our site link!'},\n",
       " {'id': 's0vufk',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'API',\n",
       "  'text': 'I’m looking for datasets or api source that quantifies fan base, or preferably, bettors’ sentiment regarding a team’s performance or direction. Does anyone know of an API that tracks this? For now I’m looking specifically for NBA, but am also interested in MLB, NFL, and NCAA f-ball and b-ball.'},\n",
       " {'id': 'rrbl7b',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'API',\n",
       "  'text': \"Hi guys,\\nI'm a new here.\\nRecently I am trying to develop something using Python.\\nWhat I need is CO2 emission data.\\nCould you guys recommend me an API of CO2 emission data?\\nPlotting CO2 emission data on y axis and x axis can be countries of the year, months, years of the country something like this.\"},\n",
       " {'id': 'rz0888',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'API',\n",
       "  'text': \"Hi,\\nWhen pulling historical data from API's I often spend a lot of time reading their documentation, and writing boilerplate code to pull their data in.\\nAs an alternative, I've built an API where you can query historical data using natural language, e.g. `w2v.get_one` or `w2v.get`\\nUnder the hood I connect with many data providers, and process their data so that each result is a single time series. I calculate sentence embeddings for each result to make them easily queryable.\\nDoes this seem interesting to anyone?\"},\n",
       " {'id': 's0l6ml',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'code',\n",
       "  'text': \"Allen Downey's python boks and videos are all excellant \\nHere is a video tutorial by him on survival analysis\\n_URL_ _URL_\\nThe notebooks\\n_URL_ _URL_\\nThe dataset on lightbulbs he uses\\n_URL_ _URL_\\nAnd his twitter\\n_URL_ _URL_\\nI have no connection with him other than liking his work.\"},\n",
       " {'id': 'rux7cw',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'dataset',\n",
       "  'text': 'Hello m8s\\nI crawled reviews of _NUMBER_ different chocolate bars as well as metadata and US and Canadian chocolate producers. \\nThe data is available here _URL_\\nAlso, the simple crawler that captured this data is available here _URL_\\nAs always, credits go to the Manhattan Chocolate Society:\\n> Manhattan Chocolate Society, Flavors of Cacao \\\\Internet\\\\]. Available from: [_URL_ _URL_ \\nHappy new year;\\nCheers'},\n",
       " {'id': 's3rbof',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'dataset',\n",
       "  'text': 'I am trying to build an app which would give me product details when i search it by name.\\nExample: I search for Apple iphone _NUMBER_\\nI should get all the informations such as description, Image, etc with it.\\nPlease let me know if there is some open source datasource available\\nPlease note that this information should not limited just to electronics good'},\n",
       " {'id': 'ryk6zd',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'dataset',\n",
       "  'text': \"This is self-promotional in a way, but it fits the philosophy here.\\nAt dolthub we're promoting our bounties to build collaborative datasets. This month we're hosting a competition to build the world's largest open dataset for housing data -- tracking it down to the sale.\\n_URL_\\nFeel free to join us on discord: _URL_\"},\n",
       " {'id': 'rvzq1k',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'dataset',\n",
       "  'text': \"Hi, what I'm looking for is pretty simple but I've been unable to get to it. I want to do a bit of a cohort analysis on covid cases. I want to know what percentage of daily covid cases are occurring among the:\\n* unvaccinated\\n* post dose _NUMBER_\\n* post dose _NUMBER_\\n* post dose _NUMBER_\\n* etc..\\nIdeally for the UK. Thanks\"},\n",
       " {'id': 's0ud6e',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'dataset',\n",
       "  'text': \"Hi r/datasets,\\nCEO of DoltHub here. We just finished our basketball database bounty called SHAQ. Here's the data if anyone wants to use it:\\n_URL_ _URL_\\nAnd here's the write up on how it did:\\n_URL_ _URL_\\nWe're on to our next data bounty, US Housing Prices, _URL_ _URL_ if the idea of getting paid to build databases intrigues you.\"},\n",
       " {'id': 's2fjbk',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'dataset',\n",
       "  'text': 'im searching video games sales since _NUMBER_ - _NUMBER_'},\n",
       " {'id': 'rwel5l',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'dataset',\n",
       "  'text': \"Is there a data set or data base out there that contains comments from social media such as tiktok or Instagram?\\nIve just browsed google about it but I haven't found anything\"},\n",
       " {'id': 'rvupxk',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'dataset',\n",
       "  'text': 'I published a dataset of 5m source code files from 15k open source files. Its long term goal is to enable identifying causality in software engineering. \\nData and code _URL_ \\nDescribing paper: End to End Software Engineering Research _URL_\\nPeople from ML, NLP, causality, and SE, might find it interesting.\\nThe dataset enables investigating code similarity , program difficulty, defect predictions, etc.\\nThe code is extracted every two months in order to investigate the difference. By the difference one can investigate if a change in a possible cause  leads to influence  _URL_ in which context. \\nI plan to keep extending the dataset and would like to get feedback on it - ease of use, new use cases, etc.\\nIf you have related dataset that can be merged with, related data that you would like to obtain or ideas for research directions, please contact me.'},\n",
       " {'id': 's6nol0',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'dataset',\n",
       "  'text': 'Hi everyone,\\nCan someone suggest to me some real-world datasets for anomaly detection? I have surfed the web enough for this and I am looking for those unique datasets for specifc domain  which have some tested anomalies.'},\n",
       " {'id': 's0ypi4',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'dataset',\n",
       "  'text': 'Scraped all public votes in German federal parliament/Bundestag . A total of _NUMBER_ voting sessions are recorded. For each of the voting sessions, the votes of each of the around _NUMBER_ parliamentary member are recorded by name of the member. Note that the voting is not strictly along the party lines. Available as excel files and zip:\\n_URL_ _URL_\\nOriginal source :\\n_URL_ _URL_'},\n",
       " {'id': 's1dit6',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'dataset',\n",
       "  'text': '\\n_URL_ _URL_\\nI have put up some  parse trees online as a dataset. Not something as substantial as Penn Treebank, since the trees are not human-edited, but still way more parse trees than from Penn to feed into your later-stage NLP algorithms, free of charge or hassle.\\nThe current format is straight from where they were generated. Suggestions of alternative formats based on ease of use would be heavily appreciated!'},\n",
       " {'id': 's0m46l',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'dataset',\n",
       "  'text': 'I am looking for dataset related to object detection to detect object after house/office has gone through fire. Do any one have lead on this kind of data are available or not , or any leads regarding this kind of system. \\nIt would be helpful if I get any lead on this.'},\n",
       " {'id': 's1g4aj',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'request',\n",
       "  'text': \"Hello everyone! So I'm in the lookout for historical documents datasets, that are preferably annotated for object detection.\\nMost of what I can find is either non annotated, or annotated for text retrieval. But in any case, please share with me anything related to the topic, it might just be useful, thank you!\"},\n",
       " {'id': 'rv7v65',\n",
       "  'subreddit': 'datasets',\n",
       "  'tag': 'request',\n",
       "  'text': \"I was wondering if there were any datasets floating around on player count for games in the pro circuit - up to date as of _NUMBER_, ideally with demographic information included - but just player count by month/day would be good enough.\\nI'm writing a paper on how gaming participation affects stadium attendance of major gaming tournaments etc.\\nAny help would be appreciated!\"}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT * from `reddit_texts.post_contents` LIMIT 20\n",
    "\"\"\"\n",
    "query_job = client.query(query)\n",
    "\n",
    "\n",
    "rows = []\n",
    "for i, row in enumerate(query_job):\n",
    "    row = dict(row)\n",
    "    text = row.pop(\"selftext\")\n",
    "    row[\"text\"] = preproc_pipe(text)\n",
    "    rows.append(row)\n",
    "\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myreddit",
   "language": "python",
   "name": "myreddit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
