{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, texts, labels):\n",
    "        self.text = texts\n",
    "        self.label = labels\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return {\"label\": self.label[key], \"text\": self.text[key]}\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    key_path = \"/Users/yco/dev/myreddit/dbt-user-creds.json\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "    client = bigquery.Client(\n",
    "        credentials=credentials,\n",
    "        project=credentials.project_id,\n",
    "    )\n",
    "    query_job = client.query(f\"SELECT * FROM `reddit_texts.posts_clean` \")\n",
    "    return Dataset(\n",
    "        [row[\"text\"] for row in query_job], [row[\"subreddit\"] for row in query_job]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "textcat_spacy = spacy.load(\n",
    "    \"../models/subreddit_classif/textcat_ens/2022/02/16/model-best\"\n",
    ")\n",
    "tokenizer_spacy = spacy.tokenizer.Tokenizer(textcat_spacy.vocab)\n",
    "\n",
    "# Run the spacy pipeline on some random text just to retrieve the classes\n",
    "doc = textcat_spacy(\"hi\")\n",
    "classes = list(doc.cats.keys())\n",
    "\n",
    "# Define a function to predict\n",
    "def predict(texts):\n",
    "    # convert texts to bare strings\n",
    "    texts = [str(text) for text in texts]\n",
    "    results = []\n",
    "    for doc in textcat_spacy.pipe(texts):\n",
    "        # results.append([{'label': cat, 'score': doc.cats[cat]} for cat in doc.cats])\n",
    "        results.append([doc.cats[cat] for cat in classes])\n",
    "    return results\n",
    "\n",
    "\n",
    "# Create a function to create a transformers-like tokenizer to match shap's expectations\n",
    "def tok_adapter(text, return_offsets_mapping=False):\n",
    "    doc = tokenizer_spacy(text)\n",
    "    out = {\"input_ids\": [tok.norm for tok in doc]}\n",
    "    if return_offsets_mapping:\n",
    "        out[\"offset_mapping\"] = [(tok.idx, tok.idx + len(tok)) for tok in doc]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Create the Shap Explainer\n",
    "# - predict is the \"model\" function, adapted to a transformers-like model\n",
    "# - masker is the masker used by shap, which relies on a transformers-like tokenizer\n",
    "# - algorithm is set to permuation, which is the one used for transformers models\n",
    "# - output_names are the classes (altough it is not propagated to the permutation explainer currently, which is why plots do not have the labels)\n",
    "# - max_evals is set to a high number to reduce the probability of cases where the explainer fails because there are too many tokens\n",
    "explainer = shap.Explainer(\n",
    "    predict,\n",
    "    masker=shap.maskers.Text(tok_adapter),\n",
    "    algorithm=\"permutation\",\n",
    "    output_names=classes,\n",
    "    max_evals=1500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.text(explainer(dataset[:3][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "from tqdm import tqdm\n",
    "\n",
    "records = []\n",
    "for sample in tqdm(dataset[:10][\"text\"]):\n",
    "    doc = textcat_spacy(sample)\n",
    "    try:\n",
    "        shap_values = explainer([sample])\n",
    "    except:\n",
    "        continue\n",
    "    predictions = {i: doc.cats[cat] for i, cat in enumerate(classes)}\n",
    "    predicted_class = max(predictions, key=lambda x: predictions[x])\n",
    "    token_attributions = [\n",
    "        rb.TokenAttributions(\n",
    "            token=token, attributions={predicted_class: values[predicted_class]}\n",
    "        )  # ignore first (CLS) and last (SEP) tokens\n",
    "        for token, values in zip(shap_values[0].data, shap_values[0].values)\n",
    "    ]\n",
    "    records.append(\n",
    "        rb.TextClassificationRecord(\n",
    "            inputs=sample,\n",
    "            prediction=[(classes[i], prob) for i, prob in predictions.items()],\n",
    "            prediction_agent=\"textcat_ens\",\n",
    "            explanation={\"text\": token_attributions},\n",
    "            multi_label=False,\n",
    "        )\n",
    "    )\n",
    "rb.delete(\"textcat_ens_explainations\")\n",
    "rb.log(records, name=\"textcat_ens_explainations\")\n",
    "\n",
    "# token_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee6d127c5ea32ef9451a868103c0a9bd2bd3e120dedba9455c4c30e8cd26a67e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('myreddit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
