{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [Jordan Boyd-Graber Course](https://www.youtube.com/watch?v=fCmIceNqVog)\n",
    "- [Gensim LDA Tutorial](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html#sphx-glr-auto-examples-tutorials-run-lda-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"reddit_data.csv\", index_col=[0]).drop(columns=[\"tag\", \"id\"])\n",
    "df = df[~df[\"text\"].isnull()]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA with nltk and gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "CUSTOM_SW = [w.lower() for w in [\"_NUMBER_\", \"_URL_\", \"_USER_\", \"_EMOJI_\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df.text.to_list()\n",
    "stoplist = list(STOPWORDS) + CUSTOM_SW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove stopwords\n",
    "docs = [[token for token in doc if token not in stoplist] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if \"_\" in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 10 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique tokens: %d\" % len(dictionary))\n",
    "print(\"Number of documents: %d\" % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha=\"auto\",\n",
    "    eta=\"auto\",\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(corpus)  # , num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print(\"Average topic coherence: %.4f.\" % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "vis_data = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim (EnsembleLDA) + NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df.text.to_list()\n",
    "stoplist = list(STOPWORDS) + CUSTOM_SW\n",
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove stopwords\n",
    "docs = [[token for token in doc if token not in stoplist] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if \"_\" in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "dictionary = Dictionary()\n",
    "for doc in docs:\n",
    "    dictionary.add_documents([[lemmatizer.lemmatize(token) for token in doc]])\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "topic_model_class = LdaModel\n",
    "\n",
    "ensemble_workers = 4\n",
    "num_models = 4\n",
    "distance_workers = 4\n",
    "num_topics = 20\n",
    "passes = 8\n",
    "from gensim.models import EnsembleLda\n",
    "\n",
    "ensemble = EnsembleLda(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    num_models=num_models,\n",
    "    topic_model_class=LdaModel,\n",
    "    ensemble_workers=ensemble_workers,\n",
    "    distance_workers=distance_workers,\n",
    ")\n",
    "\n",
    "print(len(ensemble.ttda))\n",
    "print(len(ensemble.get_topics()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"../models/topics/lda_10topics/2022/03/10/\")\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "model.save(str(path / \"model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../models/topics/lda/2022/03/10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "model_loaded = LdaModel.load(str(path / \"model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create multiple versions of the lda model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models import Phrases\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "CUSTOM_SW = [\n",
    "    w.lower()\n",
    "    for w in [\"_NUMBER_\", \"_URL_\", \"_USER_\", \"_EMOJI_\", \"ve\", \"use\", \"like\", \"work\"]\n",
    "]\n",
    "\n",
    "\n",
    "def get_model_name(num_topics, use_bigrams, bigrams_min_count, no_below, no_above):\n",
    "    return f\"lda_{num_topics}topics_{no_below}_{no_above}_{'bi'+str(bigrams_min_count) if use_bigrams else 'nobi'}\"\n",
    "\n",
    "\n",
    "def train_lda_model(\n",
    "    num_topics=10, use_bigrams=False, bigrams_min_count=20, no_below=10, no_above=0.5\n",
    "):\n",
    "    docs = df.text.to_list()\n",
    "    stoplist = list(STOPWORDS) + CUSTOM_SW\n",
    "\n",
    "    # Split the documents into tokens.\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "    # Remove stopwords\n",
    "    docs = [[token for token in doc if token not in stoplist] for doc in docs]\n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "\n",
    "    # Lemmatize the documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "    # Compute bigrams.\n",
    "\n",
    "    # Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "    if use_bigrams:\n",
    "        bigram = Phrases(docs, min_count=bigrams_min_count)\n",
    "        for idx in range(len(docs)):\n",
    "            for token in bigram[docs[idx]]:\n",
    "                if \"_\" in token:\n",
    "                    # Token is a bigram, add to document.\n",
    "                    docs[idx].append(token)\n",
    "\n",
    "    # Create a dictionary representation of the documents.\n",
    "    dictionary = Dictionary(docs)\n",
    "\n",
    "    # Filter out words that occur less than 10 documents, or more than 50% of the documents.\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    # Bag-of-words representation of the documents.\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "    # Set training parameters.\n",
    "    chunksize = 2000\n",
    "    passes = 20\n",
    "    iterations = 400\n",
    "    eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "    return (\n",
    "        LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            chunksize=chunksize,\n",
    "            alpha=\"auto\",\n",
    "            eta=\"auto\",\n",
    "            iterations=iterations,\n",
    "            num_topics=num_topics,\n",
    "            passes=passes,\n",
    "            eval_every=eval_every,\n",
    "        ),\n",
    "        corpus,\n",
    "        dictionary,\n",
    "    )\n",
    "\n",
    "\n",
    "ROOT_DIR = Path(\"..\")\n",
    "import datetime\n",
    "\n",
    "\n",
    "def save_model(name, model, corpus, dictionary):\n",
    "    date_folder = datetime.date.today().strftime(\"%Y/%m/%d\")\n",
    "    target_folder = ROOT_DIR / \"models\" / \"topics\" / name / date_folder\n",
    "    target_folder.mkdir(parents=True, exist_ok=True)\n",
    "    model.save(str(target_folder / \"model\"))\n",
    "    dictionary.save(str(target_folder / \"dictionary\"))\n",
    "    MmCorpus.serialize(str(target_folder / \"corpus\"), corpus)\n",
    "\n",
    "\n",
    "def load_model(name):\n",
    "    model_type = \"topics\"\n",
    "    config_name = name\n",
    "    model_folder = ROOT_DIR / \"models\" / model_type / config_name\n",
    "    date = \"\"\n",
    "    for _ in [\"year\", \"month\", \"day\"]:\n",
    "        date += \"/\" + max(x.name for x in model_folder.iterdir() if x.is_dir())\n",
    "    model_folder = model_folder / date\n",
    "    model = LdaModel.load(str(model_folder / \"model\"))\n",
    "    dictionary = Dictionary.load(str(model_folder / \"dictionary\"))\n",
    "    corpus = MmCorpus(str(model_folder / \"corpus\"))\n",
    "    return (\n",
    "        model,\n",
    "        dictionary,\n",
    "        corpus,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topicss = [5, 10, 20, 50]\n",
    "use_bigramss = [True, False]\n",
    "bigrams_min_counts = [5, 20]\n",
    "no_belows = [5, 10, 20, 50]\n",
    "no_aboves = [0.5, 0.99]\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "for t in product(num_topicss, use_bigramss, bigrams_min_counts, no_belows, no_aboves):\n",
    "    num_topics, use_bigrams, bigrams_min_count, no_below, no_above = t\n",
    "    name = get_model_name(\n",
    "        num_topics, use_bigrams, bigrams_min_count, no_below, no_above\n",
    "    )\n",
    "    print(name)\n",
    "    model, corpus, dictionary = train_lda_model(\n",
    "        num_topics, use_bigrams, bigrams_min_count, no_below, no_above\n",
    "    )\n",
    "    save_model(name, model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = dictionary.doc2bow([\"looking\", \"dataset\", \"pets\", \"features\", \"columns\"])\n",
    "model.get_document_topics(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee6d127c5ea32ef9451a868103c0a9bd2bd3e120dedba9455c4c30e8cd26a67e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('myreddit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
