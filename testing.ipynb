{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "auth = requests.auth.HTTPBasicAuth(\n",
    "    \"1PZH2AGk7VT9ISAAygpCAw\", \"C3rwKVVuL-L8HkJ6fiDD8wvRsqDMyg\"\n",
    ")\n",
    "login_data = {\n",
    "    \"grant_type\": \"password\",\n",
    "    \"username\": \"Qualkeck\",\n",
    "    \"password\": \"GrMkAc7Hu&\",\n",
    "}\n",
    "login_headers = {\"User-Agent\": \"MySubrs\"}\n",
    "res = requests.post(\n",
    "    \"https://www.reddit.com/api/v1/access_token\",\n",
    "    auth=auth,\n",
    "    data=login_data,\n",
    "    headers=login_headers,\n",
    ")\n",
    "token = res.json()['access_token']\n",
    "headers = {**login_headers, **{'Authorization': f\"bearer {token}\"}}\n",
    "resp = requests.get('https://oauth.reddit.com/r/LanguageTechnology/hot?limit=100', headers=headers)\n",
    "data = resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kind', 'data'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s1d4p9'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_id = data['data']['children'][3][\"data\"][\"id\"]\n",
    "post_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approved_at_utc None\n",
      "subreddit LanguageTechnology\n",
      "selftext Stanford Online students will present original projects developed in the Natural Language Understanding professional course. Q&amp;A to follow. [Register here](https://learn.stanford.edu/WBN-AI-NLU-Project-Showcase.html).\n",
      "author_fullname t2_5jtot0sw\n",
      "saved False\n",
      "mod_reason_title None\n",
      "gilded 0\n",
      "clicked False\n",
      "title Webinar: NLU Project Showcase\n",
      "subreddit_name_prefixed r/LanguageTechnology\n",
      "hidden False\n",
      "pwls 6\n",
      "link_flair_css_class None\n",
      "downs 0\n",
      "top_awarded_type None\n",
      "hide_score False\n",
      "name t3_s0vkki\n",
      "quarantine False\n",
      "link_flair_text_color dark\n",
      "upvote_ratio 0.81\n",
      "author_flair_background_color None\n",
      "subreddit_type public\n",
      "ups 3\n",
      "total_awards_received 0\n",
      "author_flair_template_id None\n",
      "is_original_content False\n",
      "secure_media None\n",
      "is_reddit_media_domain False\n",
      "is_meta False\n",
      "category None\n",
      "link_flair_text None\n",
      "can_mod_post False\n",
      "score 3\n",
      "approved_by None\n",
      "is_created_from_ads_ui False\n",
      "author_premium False\n",
      "thumbnail \n",
      "edited False\n",
      "author_flair_css_class None\n",
      "content_categories None\n",
      "is_self True\n",
      "mod_note None\n",
      "created 1641851323.0\n",
      "link_flair_type text\n",
      "wls 6\n",
      "removed_by_category None\n",
      "banned_by None\n",
      "author_flair_type text\n",
      "domain self.LanguageTechnology\n",
      "allow_live_comments False\n",
      "selftext_html &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Stanford Online students will present original projects developed in the Natural Language Understanding professional course. Q&amp;amp;A to follow. &lt;a href=\"https://learn.stanford.edu/WBN-AI-NLU-Project-Showcase.html\"&gt;Register here&lt;/a&gt;.&lt;/p&gt;\n",
      "&lt;/div&gt;&lt;!-- SC_ON --&gt;\n",
      "likes None\n",
      "suggested_sort None\n",
      "banned_at_utc None\n",
      "view_count None\n",
      "archived False\n",
      "no_follow True\n",
      "is_crosspostable True\n",
      "pinned False\n",
      "over_18 False\n",
      "media_only False\n",
      "can_gild True\n",
      "spoiler False\n",
      "locked False\n",
      "author_flair_text None\n",
      "visited False\n",
      "removed_by None\n",
      "num_reports None\n",
      "distinguished None\n",
      "subreddit_id t5_2rkr2\n",
      "author_is_blocked False\n",
      "mod_reason_by None\n",
      "removal_reason None\n",
      "link_flair_background_color \n",
      "id s0vkki\n",
      "is_robot_indexable True\n",
      "report_reasons None\n",
      "author Stanford_Online\n",
      "discussion_type None\n",
      "num_comments 0\n",
      "send_replies True\n",
      "whitelist_status all_ads\n",
      "contest_mode False\n",
      "author_patreon_flair False\n",
      "author_flair_text_color None\n",
      "permalink /r/LanguageTechnology/comments/s0vkki/webinar_nlu_project_showcase/\n",
      "parent_whitelist_status all_ads\n",
      "stickied False\n",
      "url https://www.reddit.com/r/LanguageTechnology/comments/s0vkki/webinar_nlu_project_showcase/\n",
      "subreddit_subscribers 34790\n",
      "created_utc 1641851323.0\n",
      "num_crossposts 0\n",
      "media None\n",
      "is_video False\n"
     ]
    }
   ],
   "source": [
    "article = data['data']['children'][7][\"data\"]\n",
    "article = {k: v for k, v in article.items() if not isinstance(v, dict) and not isinstance(v, list)}\n",
    "for k, v in article.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subreddit_id': 't5_2rkr2',\n",
       " 'approved_at_utc': None,\n",
       " 'author_is_blocked': False,\n",
       " 'comment_type': None,\n",
       " 'awarders': [],\n",
       " 'mod_reason_by': None,\n",
       " 'banned_by': None,\n",
       " 'author_flair_type': 'text',\n",
       " 'total_awards_received': 0,\n",
       " 'subreddit': 'LanguageTechnology',\n",
       " 'author_flair_template_id': None,\n",
       " 'likes': None,\n",
       " 'replies': {'kind': 'Listing',\n",
       "  'data': {'after': None,\n",
       "   'dist': None,\n",
       "   'modhash': None,\n",
       "   'geo_filter': '',\n",
       "   'children': [{'kind': 't1',\n",
       "     'data': {'subreddit_id': 't5_2rkr2',\n",
       "      'approved_at_utc': None,\n",
       "      'author_is_blocked': False,\n",
       "      'comment_type': None,\n",
       "      'awarders': [],\n",
       "      'mod_reason_by': None,\n",
       "      'banned_by': None,\n",
       "      'author_flair_type': 'text',\n",
       "      'total_awards_received': 0,\n",
       "      'subreddit': 'LanguageTechnology',\n",
       "      'author_flair_template_id': None,\n",
       "      'likes': None,\n",
       "      'replies': '',\n",
       "      'user_reports': [],\n",
       "      'saved': False,\n",
       "      'id': 'hs9verb',\n",
       "      'banned_at_utc': None,\n",
       "      'mod_reason_title': None,\n",
       "      'gilded': 0,\n",
       "      'archived': False,\n",
       "      'collapsed_reason_code': None,\n",
       "      'no_follow': True,\n",
       "      'author': 'l74d',\n",
       "      'can_mod_post': False,\n",
       "      'created_utc': 1641942325.0,\n",
       "      'send_replies': True,\n",
       "      'parent_id': 't1_hs88ytn',\n",
       "      'score': 1,\n",
       "      'author_fullname': 't2_cm4rlm2s',\n",
       "      'removal_reason': None,\n",
       "      'approved_by': None,\n",
       "      'mod_note': None,\n",
       "      'all_awardings': [],\n",
       "      'body': 'Good question.  Come to think of it, it is actually the other way around -- more useful in pragmatic situations than beating SOTA. In fact, if you have a task general enough for a paper as well as enough input-output pairs to compute a score with, it would always be possible to achieve better results with an end-to-end deep learning setup. The bottom line is that you can take LAL-Parser and build upon it, making strictly more information available than the discrete parse tree generated by LAL-Parser. \\n\\nHowever, when the need for a custom NLP solution (where existing tools for e.g. classification, custom NER, topic modeling, translation, etc are not directly applicable) arises in practice, more often than not there are very few outputs available unless of course investing in labeling. The training set is next to empty but the test set is infinitely large. Think extracting information according to some prescribed custom format, where the most naive solution is a Regex which might subsequently be found not powerful enough. The next step up would then be a small or just rule-based model taking as input some structured data, e.g. parse trees. The parser effectively handles the majority of \"serious NLP\" for you.',\n",
       "      'edited': 1641944158.0,\n",
       "      'top_awarded_type': None,\n",
       "      'author_flair_css_class': None,\n",
       "      'name': 't1_hs9verb',\n",
       "      'is_submitter': True,\n",
       "      'downs': 0,\n",
       "      'author_flair_richtext': [],\n",
       "      'author_patreon_flair': False,\n",
       "      'body_html': '&lt;div class=\"md\"&gt;&lt;p&gt;Good question.  Come to think of it, it is actually the other way around -- more useful in pragmatic situations than beating SOTA. In fact, if you have a task general enough for a paper as well as enough input-output pairs to compute a score with, it would always be possible to achieve better results with an end-to-end deep learning setup. The bottom line is that you can take LAL-Parser and build upon it, making strictly more information available than the discrete parse tree generated by LAL-Parser. &lt;/p&gt;\\n\\n&lt;p&gt;However, when the need for a custom NLP solution (where existing tools for e.g. classification, custom NER, topic modeling, translation, etc are not directly applicable) arises in practice, more often than not there are very few outputs available unless of course investing in labeling. The training set is next to empty but the test set is infinitely large. Think extracting information according to some prescribed custom format, where the most naive solution is a Regex which might subsequently be found not powerful enough. The next step up would then be a small or just rule-based model taking as input some structured data, e.g. parse trees. The parser effectively handles the majority of &amp;quot;serious NLP&amp;quot; for you.&lt;/p&gt;\\n&lt;/div&gt;',\n",
       "      'gildings': {},\n",
       "      'collapsed_reason': None,\n",
       "      'distinguished': None,\n",
       "      'associated_award': None,\n",
       "      'stickied': False,\n",
       "      'author_premium': False,\n",
       "      'can_gild': True,\n",
       "      'link_id': 't3_s1d4p9',\n",
       "      'unrepliable_reason': None,\n",
       "      'author_flair_text_color': None,\n",
       "      'score_hidden': False,\n",
       "      'permalink': '/r/LanguageTechnology/comments/s1d4p9/a_dataset_of_parse_trees_generated_from_abstracts/hs9verb/',\n",
       "      'subreddit_type': 'public',\n",
       "      'locked': False,\n",
       "      'report_reasons': None,\n",
       "      'created': 1641942325.0,\n",
       "      'author_flair_text': None,\n",
       "      'treatment_tags': [],\n",
       "      'collapsed': False,\n",
       "      'subreddit_name_prefixed': 'r/LanguageTechnology',\n",
       "      'controversiality': 0,\n",
       "      'depth': 1,\n",
       "      'author_flair_background_color': None,\n",
       "      'collapsed_because_crowd_control': None,\n",
       "      'mod_reports': [],\n",
       "      'num_reports': None,\n",
       "      'ups': 1}}],\n",
       "   'before': None}},\n",
       " 'user_reports': [],\n",
       " 'saved': False,\n",
       " 'id': 'hs88ytn',\n",
       " 'banned_at_utc': None,\n",
       " 'mod_reason_title': None,\n",
       " 'gilded': 0,\n",
       " 'archived': False,\n",
       " 'collapsed_reason_code': None,\n",
       " 'no_follow': True,\n",
       " 'author': 'DeaderThanElvis',\n",
       " 'can_mod_post': False,\n",
       " 'created_utc': 1641920800.0,\n",
       " 'send_replies': True,\n",
       " 'parent_id': 't3_s1d4p9',\n",
       " 'score': 2,\n",
       " 'author_fullname': 't2_4aw2x',\n",
       " 'approved_by': None,\n",
       " 'mod_note': None,\n",
       " 'all_awardings': [],\n",
       " 'collapsed': False,\n",
       " 'body': 'Great effort, thanks for sharing! My thesis was on discourse parsing so a resource like this would’ve been amazing to have back then. \\n\\nHowever, it’s been ages since I looked at a syntactic or dependency parse tree, all thanks to Transformers for making my (dream!) Linguist role obsolete in our business context — stuff like classification, custom NER, topic modeling, translation, etc. just works out of the box without any need to tinker with pos tags, chunk boundaries, and parse trees. \\n\\nSo I’m curious to know what later stage algorithms/applications would derive value from this resource. In other words, besides purely academic reasons, is there any benefit to parsing sentences anymore?',\n",
       " 'edited': False,\n",
       " 'top_awarded_type': None,\n",
       " 'author_flair_css_class': None,\n",
       " 'name': 't1_hs88ytn',\n",
       " 'is_submitter': False,\n",
       " 'downs': 0,\n",
       " 'author_flair_richtext': [],\n",
       " 'author_patreon_flair': False,\n",
       " 'body_html': '&lt;div class=\"md\"&gt;&lt;p&gt;Great effort, thanks for sharing! My thesis was on discourse parsing so a resource like this would’ve been amazing to have back then. &lt;/p&gt;\\n\\n&lt;p&gt;However, it’s been ages since I looked at a syntactic or dependency parse tree, all thanks to Transformers for making my (dream!) Linguist role obsolete in our business context — stuff like classification, custom NER, topic modeling, translation, etc. just works out of the box without any need to tinker with pos tags, chunk boundaries, and parse trees. &lt;/p&gt;\\n\\n&lt;p&gt;So I’m curious to know what later stage algorithms/applications would derive value from this resource. In other words, besides purely academic reasons, is there any benefit to parsing sentences anymore?&lt;/p&gt;\\n&lt;/div&gt;',\n",
       " 'removal_reason': None,\n",
       " 'collapsed_reason': None,\n",
       " 'distinguished': None,\n",
       " 'associated_award': None,\n",
       " 'stickied': False,\n",
       " 'author_premium': False,\n",
       " 'can_gild': True,\n",
       " 'gildings': {},\n",
       " 'unrepliable_reason': None,\n",
       " 'author_flair_text_color': None,\n",
       " 'score_hidden': False,\n",
       " 'permalink': '/r/LanguageTechnology/comments/s1d4p9/a_dataset_of_parse_trees_generated_from_abstracts/hs88ytn/',\n",
       " 'subreddit_type': 'public',\n",
       " 'locked': False,\n",
       " 'report_reasons': None,\n",
       " 'created': 1641920800.0,\n",
       " 'author_flair_text': None,\n",
       " 'treatment_tags': [],\n",
       " 'link_id': 't3_s1d4p9',\n",
       " 'subreddit_name_prefixed': 'r/LanguageTechnology',\n",
       " 'controversiality': 0,\n",
       " 'depth': 0,\n",
       " 'author_flair_background_color': None,\n",
       " 'collapsed_because_crowd_control': None,\n",
       " 'mod_reports': [],\n",
       " 'num_reports': None,\n",
       " 'ups': 2}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.get(f'https://oauth.reddit.com/r/LanguageTechnology/comments/{post_id}?limit=100&depth=10', headers=headers)\n",
    "comments_data = resp.json()[1][\"data\"][\"children\"][0][\"data\"]\n",
    "comments_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['kind', 'data'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wf/mfxmf_mx3_s4flxsfp01bntw0000gn/T/ipykernel_29542/3970038140.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"parent_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"replies\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"children\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"parent_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "print(data.keys())\n",
    "comments_data[\"id\"], comments_data[\"parent_id\"], comments_data[\"replies\"][\"data\"][\"children\"][0][\"data\"][\"parent_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Congrats on the offer! I'm a Meta DE, and my WLB is pretty good, but if you're new to big tech it may not be at first.\\n\\nThat's because at Meta, you're expected to find your own projects and ways to contribute impact to your team. You will have a manger that will act as a sounding board and a coach to help you grow, but mostly it all comes from you. No-one is going to tell you what to do, and if they do, you're free to say no.\\n\\nAdditionally, as DEs, some of the impact that we make doesn't become apparent for quite a while after we do the work. Built a new table? It might be weeks until people start to use and gain regular value for it. depending on their own priorities. Wrote a bunch of wikis on how to use your teams data? The impact won't be apparent for months maybe, when you realise that you've stopped getting so many ad-hoc questions, or that more people are accessing your tables.\\n\\nSo because of this, new hires, and especially people who aren't used to this self-directed way of working, may get themselves into a panic that they are not being impactful enough, and end up working extra to over-deliver.\\n\\nBut, as you get used to the role, you will -\\n\\n\\\\- Develop a greater sense for what you can do to add impact, and learn that this doesn't always correlate to effort.\\n\\n\\\\- Learn about deferred impact, and understand the long-term effects you can have on your team, instead of putting all your effort into short-term projects.\\n\\n\\\\- Understand the expectations of your role better, and what you need to do to get to the next level (if you want to!).\\n\\nFor me personally, I was doing around 50-60 hours a week because I was terrified that I wasn't doing enough. Having been here for two years, I've got that down to a reasonable 40 hours. Very occasionally that may stretch to be closer to 50 if I'm crunching on an expected high-impact project, or get pulled into a big issue during oncall, but it's pretty rare.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.json()[1][\"data\"][\"children\"][0][\"data\"][\"body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataengineering\n",
      "datasets\n",
      "LanguageTechnology\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import requests\n",
    "auth = requests.auth.HTTPBasicAuth(\n",
    "    \"1PZH2AGk7VT9ISAAygpCAw\", \"C3rwKVVuL-L8HkJ6fiDD8wvRsqDMyg\"\n",
    ")\n",
    "login_data = {\n",
    "    \"grant_type\": \"password\",\n",
    "    \"username\": \"Qualkeck\",\n",
    "    \"password\": \"GrMkAc7Hu&\",\n",
    "}\n",
    "login_headers = {\"User-Agent\": \"MySubrs\"}\n",
    "res = requests.post(\n",
    "    \"https://www.reddit.com/api/v1/access_token\",\n",
    "    auth=auth,\n",
    "    data=login_data,\n",
    "    headers=login_headers,\n",
    ")\n",
    "token = res.json()['access_token']\n",
    "headers = {**login_headers, **{'Authorization': f\"bearer {token}\"}}\n",
    "MY_SUBREDDITS = [\n",
    "    \"dataengineering\",\n",
    "    \"datasets\",\n",
    "    \"LanguageTechnology\",\n",
    "]\n",
    "date = datetime.date.today()\n",
    "raw_data = []\n",
    "for subreddit in MY_SUBREDDITS:\n",
    "    print(subreddit)\n",
    "    resp = requests.get(\n",
    "        f\"https://oauth.reddit.com/r/{subreddit}/hot?limit=100\", headers=headers\n",
    "    ).json()\n",
    "    for article in resp[\"data\"][\"children\"]:\n",
    "        article = {k: v for k, v in article[\"data\"].items() if not isinstance(v, dict) and not isinstance(v, list)}\n",
    "        raw_data.append({\"fetch_date\": date, \"subreddit\": subreddit, **article})\n",
    "\n",
    "df_posts = pd.DataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fetch_date                        object\n",
       "subreddit                         object\n",
       "approved_at_utc                   object\n",
       "selftext                          object\n",
       "author_fullname                   object\n",
       "saved                               bool\n",
       "mod_reason_title                  object\n",
       "gilded                             int64\n",
       "clicked                             bool\n",
       "title                             object\n",
       "subreddit_name_prefixed           object\n",
       "hidden                              bool\n",
       "pwls                               int64\n",
       "link_flair_css_class              object\n",
       "downs                              int64\n",
       "thumbnail_height                 float64\n",
       "top_awarded_type                  object\n",
       "hide_score                          bool\n",
       "name                              object\n",
       "quarantine                          bool\n",
       "link_flair_text_color             object\n",
       "upvote_ratio                     float64\n",
       "author_flair_background_color     object\n",
       "ups                                int64\n",
       "total_awards_received              int64\n",
       "thumbnail_width                  float64\n",
       "author_flair_template_id          object\n",
       "is_original_content                 bool\n",
       "secure_media                     float64\n",
       "is_reddit_media_domain              bool\n",
       "is_meta                             bool\n",
       "category                          object\n",
       "link_flair_text                   object\n",
       "can_mod_post                        bool\n",
       "score                              int64\n",
       "approved_by                       object\n",
       "is_created_from_ads_ui              bool\n",
       "author_premium                    object\n",
       "thumbnail                         object\n",
       "edited                            object\n",
       "author_flair_css_class            object\n",
       "post_hint                         object\n",
       "content_categories                object\n",
       "is_self                             bool\n",
       "subreddit_type                    object\n",
       "created                          float64\n",
       "link_flair_type                   object\n",
       "wls                                int64\n",
       "removed_by_category               object\n",
       "banned_by                         object\n",
       "author_flair_type                 object\n",
       "domain                            object\n",
       "allow_live_comments                 bool\n",
       "selftext_html                     object\n",
       "likes                             object\n",
       "suggested_sort                    object\n",
       "banned_at_utc                     object\n",
       "url_overridden_by_dest            object\n",
       "view_count                        object\n",
       "archived                            bool\n",
       "no_follow                           bool\n",
       "is_crosspostable                    bool\n",
       "pinned                              bool\n",
       "over_18                             bool\n",
       "media_only                          bool\n",
       "link_flair_template_id            object\n",
       "can_gild                            bool\n",
       "spoiler                             bool\n",
       "locked                              bool\n",
       "author_flair_text                 object\n",
       "visited                             bool\n",
       "removed_by                        object\n",
       "mod_note                          object\n",
       "distinguished                     object\n",
       "subreddit_id                      object\n",
       "author_is_blocked                   bool\n",
       "mod_reason_by                     object\n",
       "num_reports                       object\n",
       "removal_reason                    object\n",
       "link_flair_background_color       object\n",
       "id                                object\n",
       "is_robot_indexable                  bool\n",
       "report_reasons                    object\n",
       "author                            object\n",
       "discussion_type                   object\n",
       "num_comments                       int64\n",
       "send_replies                        bool\n",
       "whitelist_status                  object\n",
       "contest_mode                        bool\n",
       "author_patreon_flair              object\n",
       "author_flair_text_color           object\n",
       "permalink                         object\n",
       "parent_whitelist_status           object\n",
       "stickied                            bool\n",
       "url                               object\n",
       "subreddit_subscribers              int64\n",
       "created_utc                      float64\n",
       "num_crossposts                     int64\n",
       "media                            float64\n",
       "is_video                            bool\n",
       "author_cakeday                    object\n",
       "crosspost_parent                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = None\n",
    "df_posts.dtypes"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee6d127c5ea32ef9451a868103c0a9bd2bd3e120dedba9455c4c30e8cd26a67e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('myreddit': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
